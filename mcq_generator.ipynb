{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these in the terminal directory where the main.ipynb file is located:\n",
    "    wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
    "    tar -xvf  s2v_reddit_2015_md.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/michaelmbajwa/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import operator\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.similarity import max_similarity\n",
    "from flashtext import KeywordProcessor\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')\n",
    "# File system manangement\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load sense2vec vectors\n",
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk('s2v_old')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Tuple\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1- Import the text file/article that has to be used for MCQ generation\n",
    "\n",
    "file = open(\"article2.txt\",\"r\") #\"r\" deontes read version open\n",
    "text = file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Stopwords are the words that we need to avoid while considering keyword extraction\n",
    "import string\n",
    "\n",
    "def find_important_word(model, article): \n",
    "    extractor=model()\n",
    "    extractor.load_document(input=article,language='en')\n",
    "    pos={'PROPN'} #We are only considering proper nouns as valid candidates for our keywords\n",
    "    extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
    "    stops=list(string.punctuation) #Stoplist contains the words to be avoided\n",
    "    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-'] #These stand for the brackets as in lrb=left round bracket=\"(\" and so on\n",
    "    stops+=stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos) #Sets the candidate selection criteria, as in, which should be considered and which should be avoided\n",
    "    extractor.candidate_weighting() #Sets the preference criteria for the candidates\n",
    "    result=[] \n",
    "    ex=extractor.get_n_best(n=15) #Gets the 15 best candidates according to the criteria set\n",
    "    for each in ex:\n",
    "        result.append(each[0]) \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pke.unsupervised import *\n",
    "outputs = {}\n",
    "\n",
    "# We use different unsupervised machine learning algorithms\n",
    "for model in [FirstPhrases, TextRank, SingleRank, TopicRank, MultipartiteRank]:\n",
    "    important_words=find_important_word(model, text)\n",
    "    outputs[model.__name__] = important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to explore the result of each algorithm\n",
    "for key, value in outputs.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all key words generated from the 5 unsupervised models used\n",
    "all_keywords = []\n",
    "for k, v in outputs.items():\n",
    "    all_keywords.extend(v)\n",
    "\n",
    "# drop duplicates\n",
    "important_words = list(set(all_keywords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the whole text article into an array/list of individual sentences. This will help us fetch the sentences related to the keywords easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def splitTextToSents(article, n):\n",
    "    s=[sent_tokenize(article)]\n",
    "    s=[y for x in s for y in x]\n",
    "    s=[sent.strip() for sent in s if len(sent)>n] #Removes all the sentences that have length less than 15 so that we can ensure that our questions have enough length for context\n",
    "    return s\n",
    "sents=splitTextToSents(text, n=15) #Achieve a well splitted set of sentences from the text article\n",
    "#print(sents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the sentences which contain the keywords to the related keywords so that we can easily lookup the sentences related to the keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def map_sentence(important_words,sents):\n",
    "    our_sents = list(set(sents)) # to remove duplicates\n",
    "    processor=KeywordProcessor() #Using keyword processor as our processor for this task\n",
    "    keySents={}\n",
    "    \n",
    "    for word in important_words:\n",
    "        keySents[word]=set() #set for avoid duplicates\n",
    "        processor.add_keyword(word) #Adds key word to the processor\n",
    "    \n",
    "    for sent in our_sents:\n",
    "        found = processor.extract_keywords(sent) #Extract the keywords in the sentence\n",
    "        \n",
    "        # we select only one found keyword. Else questions will be similar\n",
    "        if len(found) > 0:\n",
    "            each = found[0]\n",
    "            keySents[each].add(sent)\n",
    "    \n",
    "    for ky, val in keySents.items():\n",
    "        keySents[ky] = list(val)\n",
    "    \n",
    "    for key in keySents.keys():\n",
    "        temp=keySents[key]\n",
    "        temp=sorted(temp,key=len,reverse=True) #Sort the sentences according to their decreasing length in order to ensure the quality of question for the MCQ \n",
    "        keySents[key]=temp\n",
    "    return keySents\n",
    "\n",
    "mapped_sentences=map_sentence(important_words,sents) # Extract the sentences that contain the keywords and map those sentences to the keywords using this function\n",
    "\n",
    "#print(mapped_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distractor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1 for generating distractors\n",
    "def get_answer_and_distractor_embeddings(answer,candidate_distractors):\n",
    "    model= SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    answer_embedding = model.encode([answer])\n",
    "    distractor_embeddings = model.encode(candidate_distractors)\n",
    "    return answer_embedding,distractor_embeddings\n",
    "\n",
    "\n",
    "def mmr(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words: List[str],\n",
    "        top_n: int = 5,\n",
    "        diversity: float = 0.9) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Maximal Marginal Relevance (MMR)\n",
    "    between candidate keywords.\n",
    "\n",
    "\n",
    "    MMR considers the similarity of keywords/keyphrases with the\n",
    "    document, along with the similarity of already selected\n",
    "    keywords and keyphrases. This results in a selection of keywords\n",
    "    that maximize their within diversity with respect to the document.\n",
    "\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]\n",
    "\n",
    "\n",
    "def get_distractors_sense2vec(originalword):\n",
    "    # generates distractors for words using sense2vec package\n",
    "    word = originalword.lower()\n",
    "    word = word.replace(\" \", \"_\")\n",
    "    sense = s2v.get_best_sense(word)\n",
    "    # function is terminated if sense2vec cannot get a sense of the word. In cases such as this, we use the second algorithm\n",
    "    if not sense:\n",
    "        return None\n",
    "    # Get the most similar words, these are the distractors\n",
    "    most_similar = s2v.most_similar(sense, n=20)\n",
    "    \n",
    "    distractors = []\n",
    "    # clean the distractors and return the result\n",
    "    for each_word in most_similar:\n",
    "        append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \")\n",
    "        if append_word not in distractors and append_word != originalword:\n",
    "            distractors.append(append_word)\n",
    "    distractors.insert(0,originalword)\n",
    "    return distractors\n",
    "\n",
    "\n",
    "def best_distractors_sense2vec(originalword):\n",
    "    distractors = get_distractors_sense2vec(originalword)\n",
    "    if not distractors:\n",
    "        return None\n",
    "    answer_embedd, distractor_embedds = get_answer_and_distractor_embeddings(originalword,distractors)\n",
    "    \n",
    "    final_distractors = mmr(answer_embedd,distractor_embedds,distractors,len(distractors)-1)\n",
    "    filtered_distractors = []\n",
    "    for dist in final_distractors:\n",
    "        filtered_distractors.append(dist[0])\n",
    "    Filtered_Distractors =  filtered_distractors[1:len(filtered_distractors)-1]\n",
    "    return Filtered_Distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 for generating distractors\n",
    "# Get the sense of the word. In order to attain a quality set of distractors we need to get the right sense of the keyword. \n",
    "def get_sense_word(sent,word):\n",
    "    word=word.lower() \n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    synsets=wn.synsets(word,'n') #Sysnets from Google are invoked\n",
    "    if synsets:\n",
    "        wup=max_similarity(sent,word,'wup',pos='n')\n",
    "        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Get distractor from WordNet. These distractors work on the basis of hypernym and hyponym explained in detail in the documentation.\n",
    "def find_distractors(syn,word):\n",
    "    dists=[]\n",
    "    word=word.lower()\n",
    "    actword=word\n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms() #Gets the hypernyms of the word\n",
    "    if len(hypernym)==0: #If there are no hypernyms for the current word, we simple return the empty list of distractors\n",
    "        return dists\n",
    "    for each in hypernym[0].hyponyms(): #Otherwise we find the relevant hyponyms for the hypernyms\n",
    "        name=each.lemmas()[0].name()\n",
    "        if(name==actword):\n",
    "            continue\n",
    "        name=name.replace(\"_\",\" \")\n",
    "        name=\" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in dists: #If the word is not already present in the list and is different from the actual word\n",
    "            dists.append(name)\n",
    "    return dists\n",
    "\n",
    "# The primary goal of this step is to take our MCQ quality one step further. The WordNet might some times fail to produce a hypernym \n",
    "# for some words. In that case the ConcepNet comes into play as they help achieve our distractors when there are no hypernyms \n",
    "# present for it in the WordNet.\n",
    "def find_distractors2(word):\n",
    "    word=word.lower()\n",
    "    actword=word\n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    dists=[]\n",
    "    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word) #To get ditractors from ConceptNet's API\n",
    "    obj=requests.get(url).json()\n",
    "    for edge in obj['edges']:\n",
    "        link=edge['end']['term']\n",
    "        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2=requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2=edge['start']['label']\n",
    "            if word2 not in dists and actword.lower() not in word2.lower(): #If the word is not already present in the list and is different from he actial word\n",
    "                dists.append(word2)\n",
    "    return dists\n",
    "\n",
    "# ranks the distracts by the similarity to the key word\n",
    "def rank_distractors_word_similarity(main_word, distractors):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    g1 = nlp(main_word)\n",
    "    r = {}\n",
    "    for v in distractors:\n",
    "        g2 = nlp(v)\n",
    "        r[v] = g1.similarity(g2)\n",
    "    result = []\n",
    "    sorted_r = dict(sorted(r.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    for dist, _ in sorted_r.items():\n",
    "        result.append(dist)\n",
    "    return result\n",
    "\n",
    "\n",
    "def hypernym_hyponym_distractors(sentence, word):\n",
    "    wordsense = get_sense_word(sentence, word)\n",
    "    if wordsense: # if the wordsense is not null/none\n",
    "        dists=find_distractors(wordsense, word) # Gets the WordNet distractors\n",
    "        if len(dists)==0: # If there are no WordNet distractors available for the current word\n",
    "            dists=find_distractors2(word) # The gets the distractors from the ConceptNet API\n",
    "        if len(dists)!=0: # If there are indeed distractors from WordNet available, \n",
    "            final_dists = rank_distractors_word_similarity(word, dists)\n",
    "            return final_dists\n",
    "    else: #If there is no wordsense, the directly searches/uses the ConceptNet\n",
    "        dists = find_distractors2(word)\n",
    "        if len(dists)>0:\n",
    "            final_dists = rank_distractors_word_similarity(word, dists)\n",
    "            return final_dists\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_distractors(sentence, word):\n",
    "    distractor_mapper = {}\n",
    "    dists = best_distractors_sense2vec(word)    \n",
    "    if not dists:\n",
    "        dists = hypernym_hyponym_distractors(sentence, word)\n",
    "    distractor_mapper[word] = dists\n",
    "    return distractor_mapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_name = file.name\n",
    "output_file_name = file.name.split(\".\")[0] + \"_MCQs.txt\"\n",
    "\n",
    "# Delete the file if it exists (this occurs if the code has been run before for similar article)\n",
    "if os.path.exists(output_file_name):\n",
    "      os.remove(output_file_name)\n",
    "else:\n",
    "  pass \n",
    "  \n",
    "with open(output_file_name, 'w') as f:\n",
    "    f.write(\"Multiple Choice Questions generated for the provided article: {0}.\".format(file.name))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    iterator = 1 #To keep the count of the questions\n",
    "    for each in mapped_sentences:\n",
    "        sentences = mapped_sentences[each]\n",
    "        for sent in sentences:\n",
    "            distractor_mapper = map_distractors(sent, each)\n",
    "            reg_compile = r'\\b' + each + r'\\b'\n",
    "            p=re.compile(reg_compile, re.IGNORECASE)\n",
    "            op=p.sub(\"________\",sent) # Replaces the keyword with underscores(blanks)\n",
    "            number_question = \"Question %s ->\"%(iterator) + \" \" + op\n",
    "            f.write(number_question) # writes the question along with a question number\n",
    "            options=[each.capitalize()] + distractor_mapper[each] # Capitalizes the options\n",
    "            options=options[:5] # Selects only 5 options\n",
    "            opts=['a','b','c','d', 'e']\n",
    "            random.shuffle(options) # Shuffle the options so that order is not always same\n",
    "            for i,ch in enumerate(options):\n",
    "                f.write(\"\\n\"+ \"\\t\" + opts[i] + \") \" + ch)\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            iterator+=1 #Increase the counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a2e03e9256ad0d3604df0aee6b872ec8994ce4b066d9871d89ee92775d315bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
